# converting to the xts object
CCI_xts <- xts(CCI[,7], order.by=CCI$TIME)
View(CCI_xts)
# checking and dealing with NA
colSums(is.na(CCI))
## aggregating the data to the quarterly frequency
CCI_q <- to.quarterly(CCI_xts)
View(CCI_xts)
View(CCI_q)
CCI_q <- CCI_q[,4]
names(CCI_q) <- c("CCI_q")
View(CCI_q)
# taking the first difference
CCI_q_diff <- diff.xts(CCI_q)
View(CCI_q_diff)
CCI_q_diff <- CCI_q_diff[-1,]
View(CCI_q_diff)
getSymbols("^SP500-40",
from = "1993-01-01")
#  scrapping data for dependent variable from yahoo finance
getSymbols("^SP500-40",
from = "1993-01-01")
# grapping closed prices
SP500_banks <- `SP500-40`[,4]
names(SP500_banks) <- c("close")
## dealing with NA values
# check how many NA I have
colSums(is.na(SP500_banks))
# replacing the missing values with the most recent observation
SP500_banks <- na.locf(SP500_banks)
## basic statistic of the bank prices
hist(SP500_banks)
summary(SP500_banks)
dev.off()
chart_Series(SP500_banks$close)
# aggregating the data to the quarterly frequency
SP500_banks_q <- to.quarterly(SP500_banks)
SP500_banks_q <- SP500_banks_q[,4]
names(SP500_banks_q) <- c("close")
str(SP500_banks_q)
# taking 1st difference to deal with unit root
SP500_banks_diff <- diff.xts(log(SP500_banks_q))
SP500_banks_diff <- SP500_banks_diff[-1,]
hist(SP500_banks_diff)
chart_Series(SP500_banks_diff$close)
## ADF test
# H_0: time series has unit root -> non-stationary
# original data -> data is non-stationary
adf_test_banks <- ur.df(SP500_banks_q$close,
type = c("drift"),
lags = 1)
summary(adf_test)
summary(adf_test_banks)
# after the 1st difference -> data is stationary
adf_test_banks_diff <- ur.df(SP500_banks_diff$close,
type = c("drift"),
lags = 1)
summary(adf_test_diff)
summary(adf_test_banks_diff)
# quick comparison
summary(adf_test_diff)@teststat[1]
CPI <- read.csv("/Users/yuriisemenov/Documents/UW/Empirical Finance/Data/CPI.csv",
sep = ",")
str(CPI)
# dealing with data format
CPI$DATE <- as.POSIXct(CPI$DATE, format="%Y-%m-%d")
# converting to the xts object
CPI_xts <- xts(CPI[,2], order.by=CPI$DATE)
# checking and dealing with NA
colSums(is.na(CPI))
## aggregating the data to the quarterly frequency
CPI_q <- to.quarterly(CPI_xts)
CPI_q <- CPI_q[,4]
names(CPI_q) <- c("CPI_q")
# taking the first difference
CPI_q_diff <- diff.xts(CPI_q)
CPI_q_diff <- CPI_q_diff[-1,]
## ADF test
# H_0: time series has unit root -> non-stationary
# original data -> data is non-stationary
adf_test_CPI <- ur.df(CPI_q$CPI_q,
type = c("drift"),
lags = 1)
summary(adf_test_CPI)
# 1st diff -> data is stationary
adf_test_CPI_diff <- ur.df(CPI_q_diff$CPI_q,
type = c("drift"),
lags = 1)
summary(adf_test_CPI_diff)
# consumer confidence index
CCI <- read_excel("/Users/yuriisemenov/Documents/UW/Empirical Finance/Data/CCI.xls")
str(CCI)
# dealing with data format
CCI$TIME <- as.POSIXct(CCI$TIME, format="%Y-%m-%d")
# converting to the xts object
CCI_xts <- xts(CCI[,7], order.by=CCI$TIME)
# checking and dealing with NA
colSums(is.na(CCI))
## aggregating the data to the quarterly frequency
CCI_q <- to.quarterly(CCI_xts)
CCI_q <- CCI_q[,4]
names(CCI_q) <- c("CCI_q")
# taking the first difference
CCI_q_diff <- diff.xts(CCI_q)
CCI_q_diff <- CCI_q_diff[-1,]
## ADF test
# H_0: time series has unit root -> non-stationary
# original data -> data is non-stationary
adf_test_CCI <- ur.df(CCI_q$CCI_q,
type = c("drift"),
lags = 1)
summary(adf_test_CCI)
# 1st diff -> data is stationary
adf_test_CCI_diff <- ur.df(CCI_q_diff$CCI_q,
type = c("drift"),
lags = 1)
summary(adf_test_CCI_diff)
################################################################################
## LIQUDITY
################################################################################
#  scrapping data for dependent variable from yahoo finance
getSymbols("^GSPC",
from = "1993-01-01")
# grapping closed prices
SP500_liq <- GSPC[,5]
names(SP500_liq) <- c("trade_volume")
## dealing with NA values
# check how many NA there are in the file
colSums(is.na(SP500_liq))
## basic statistic of the bank prices
hist(SP500_liq)
summary(SP500_liq)
dev.off()
chart_Series(SP500_liq$trade_volume)
# aggregating the data to the quarterly frequency
SP500_liq_q <- to.quarterly(SP500_liq)
SP500_liq_q <- SP500_liq_q[,4]
names(SP500_liq_q) <- c("trade_volume")
str(SP500_banks_q)
# taking 1st difference to deal with unit root
SP500_liq_diff <- diff.xts(SP500_banks_q)
SP500_liq_diff <- SP500_liq_diff[-1,]
names(SP500_liq_diff) <- c("trade_volume")
hist(SP500_liq_diff)
chart_Series(SP500_liq_diff$trade_volume)
## ADF test
# H_0: time series has unit root -> non-stationary
# original data -> data is non-stationary
adf_test_liq <- ur.df(SP500_liq_q$trade_volume,
type = c("drift"),
lags = 1)
summary(adf_test_liq)
# after the 1st difference -> data is stationary
adf_test_liq_diff <- ur.df(SP500_liq_diff$trade_volume,
type = c("drift"),
lags = 1)
summary(adf_test_liq_diff)
# merging the data
data_set <- merge(SP500_banks_diff, CPI_q_diff, SP500_liq_diff, CCI_q_diff)
View(data_set)
# removing NA
data_set <- data_set[(-1:2),]
# removing NA
data_set <- data_set[(-1:-2),]
View(data_set)
# removing NA
tail(data_set)
data_set <- data_set[-120,]
# removing NA
tail(data_set)
# names for columns
names(data_set) <- c( "prices","CPI", "trade_volume", "CCI")
adf_test <- list(adf_bank_0 = adf_test_banks,
adf_bank_1 = adf_test_banks_diff,
adf_CPI_0 = adf_test_CPI,
adf_CPI_1 = adf_test_CPI_diff,
adf_liq_0 = adf_test_liq,
adf_liq_1 = adf_test_liq_diff,
adf_CCI_0 = adf_test_CCI,
adf_CCI_1 = adf_test_CCI_diff)
adf_test <- list(adf_bank_0 = adf_test_banks,
adf_bank_1 = adf_test_banks_diff,
adf_CPI_0 = adf_test_CPI,
adf_CPI_1 = adf_test_CPI_diff,
adf_liq_0 = adf_test_liq,
adf_liq_1 = adf_test_liq_diff,
adf_CCI_0 = adf_test_CCI,
adf_CCI_1 = adf_test_CCI_diff)
# removing adf tests
rm(adf_test_banks_diff,adf_test_CPI, adf_test_CPI_diff, adf_test_liq,
adf_test_liq_diff,adf_test_banks,adf_test_CCI, adf_test_CCI_diff)
# removing raw data
rm(GSPC,CPI, CPI_q, CPI_q_diff, CPI_xts, SP500_banks, `SP500-40`, SP500_banks_diff,
SP500_banks_q, SP500_liq, SP500_liq_diff, SP500_liq_q, CCI, CCI_q, CCI_q_diff,
CCI_xts)
# Pearson correlation
pearson_corr <-cor(data_set)
pearson_corr
# selecting number of lags in the model
VARselect(data_set, # input data
lag.max = 6)    # maximum lag
# running the VAR model
data_set_var2 <- VAR(data_set,
p = 2,  # order of VAR model
season = 12)
summary(data_set_var2)
# running the VAR model
data_set_var2 <- VAR(data_set,
p = 2)
summary(data_set_var2)
# selecting number of lags in the model
VARselect(data_set, # input data
lag.max = 12)    # maximum lag
pearson_corr
# uncertainty index
EPU <- read_excel("/Users/yuriisemenov/Documents/UW/Empirical Finance/Data/EPU.xls")
# uncertainty index
EPU <- read_excel("/Users/yuriisemenov/Documents/UW/Empirical Finance/Data/EPU.xlsx")
View(EPU)
# uncertainty index
EPU <- read_excel("/Users/yuriisemenov/Documents/UW/Empirical Finance/Data/EPU.xls")
View(EPU)
# dealing with data format
EPU$DATE <- as.POSIXct(EPU$DATE, format="%Y-%m-%d")
View(EPU)
# dealing with data format
EPU$DATE <- as.POSIXct(EPU$Date, format="%Y-%m-%d")
str(EPU)
View(EPU)
# converting to the xts object
EPU_xts <- xts(EPU[,2], order.by=EPU$Date)
# checking and dealing with NA
colSums(is.na(EPU))
## aggregating the data to the quarterly frequency
EPU_q <- to.quarterly(EPU_xts)
View(EPU_xts)
View(EPU_q)
EPU_q <- EPU_q[,4]
names(EPU_q) <- c("EPU_q")
# taking the first difference
EPU_q_diff <- diff.xts(EPU_q)
View(EPU_q_diff)
EPU_q_diff <- EPU_q_diff[-1,]
View(EPU_q_diff)
# merging the data
data_set <- merge(SP500_banks_diff, CPI_q_diff, SP500_liq_diff, CCI_q_diff,
EPU_q_diff)
getSymbols("^SP500-40",
from = "1993-01-01")
# grapping closed prices
SP500_banks <- `SP500-40`[,4]
names(SP500_banks) <- c("close")
## dealing with NA values
# check how many NA I have
colSums(is.na(SP500_banks))
# replacing the missing values with the most recent observation
SP500_banks <- na.locf(SP500_banks)
## basic statistic of the bank prices
hist(SP500_banks)
summary(SP500_banks)
dev.off()
chart_Series(SP500_banks$close)
# aggregating the data to the quarterly frequency
SP500_banks_q <- to.quarterly(SP500_banks)
SP500_banks_q <- SP500_banks_q[,4]
names(SP500_banks_q) <- c("close")
str(SP500_banks_q)
# taking 1st difference to deal with unit root
SP500_banks_diff <- diff.xts(log(SP500_banks_q))
SP500_banks_diff <- SP500_banks_diff[-1,]
hist(SP500_banks_diff)
chart_Series(SP500_banks_diff$close)
## ADF test
# H_0: time series has unit root -> non-stationary
# original data -> data is non-stationary
adf_test_banks <- ur.df(SP500_banks_q$close,
type = c("drift"),
lags = 1)
summary(adf_test_banks)
# after the 1st difference -> data is stationary
adf_test_banks_diff <- ur.df(SP500_banks_diff$close,
type = c("drift"),
lags = 1)
summary(adf_test_banks_diff)
# # quick comparison
# summary(adf_test_diff)@teststat[1]
# summary(adf_test_diff)@cval
################################################################################
## MACRO VARIABLES (inflation, confidence index, uncertainty index)
################################################################################
# consumer price index
CPI <- read.csv("/Users/yuriisemenov/Documents/UW/Empirical Finance/Data/CPI.csv",
sep = ",")
str(CPI)
# dealing with data format
CPI$DATE <- as.POSIXct(CPI$DATE, format="%Y-%m-%d")
# converting to the xts object
CPI_xts <- xts(CPI[,2], order.by=CPI$DATE)
# checking and dealing with NA
colSums(is.na(CPI))
## aggregating the data to the quarterly frequency
CPI_q <- to.quarterly(CPI_xts)
CPI_q <- CPI_q[,4]
names(CPI_q) <- c("CPI_q")
# taking the first difference
CPI_q_diff <- diff.xts(CPI_q)
CPI_q_diff <- CPI_q_diff[-1,]
## ADF test
# H_0: time series has unit root -> non-stationary
# original data -> data is non-stationary
adf_test_CPI <- ur.df(CPI_q$CPI_q,
type = c("drift"),
lags = 1)
summary(adf_test_CPI)
# 1st diff -> data is stationary
adf_test_CPI_diff <- ur.df(CPI_q_diff$CPI_q,
type = c("drift"),
lags = 1)
summary(adf_test_CPI_diff)
# consumer confidence index
CCI <- read_excel("/Users/yuriisemenov/Documents/UW/Empirical Finance/Data/CCI.xls")
str(CCI)
# dealing with data format
CCI$TIME <- as.POSIXct(CCI$TIME, format="%Y-%m-%d")
# converting to the xts object
CCI_xts <- xts(CCI[,7], order.by=CCI$TIME)
# checking and dealing with NA
colSums(is.na(CCI))
## aggregating the data to the quarterly frequency
CCI_q <- to.quarterly(CCI_xts)
CCI_q <- CCI_q[,4]
names(CCI_q) <- c("CCI_q")
# taking the first difference
CCI_q_diff <- diff.xts(CCI_q)
CCI_q_diff <- CCI_q_diff[-1,]
## ADF test
# H_0: time series has unit root -> non-stationary
# original data -> data is non-stationary
adf_test_CCI <- ur.df(CCI_q$CCI_q,
type = c("drift"),
lags = 1)
summary(adf_test_CCI)
# 1st diff -> data is stationary
adf_test_CCI_diff <- ur.df(CCI_q_diff$CCI_q,
type = c("drift"),
lags = 1)
summary(adf_test_CCI_diff)
# uncertainty index
EPU <- read_excel("/Users/yuriisemenov/Documents/UW/Empirical Finance/Data/EPU.xls")
str(EPU)
# dealing with data format
EPU$DATE <- as.POSIXct(EPU$Date, format="%Y-%m-%d")
# converting to the xts object
EPU_xts <- xts(EPU[,2], order.by=EPU$Date)
# checking and dealing with NA
colSums(is.na(EPU))
## aggregating the data to the quarterly frequency
EPU_q <- to.quarterly(EPU_xts)
EPU_q <- EPU_q[,4]
names(EPU_q) <- c("EPU_q")
# taking the first difference
EPU_q_diff <- diff.xts(EPU_q)
EPU_q_diff <- EPU_q_diff[-1,]
################################################################################
## LIQUDITY
################################################################################
#  scrapping data for dependent variable from yahoo finance
getSymbols("^GSPC",
from = "1993-01-01")
# grapping closed prices
SP500_liq <- GSPC[,5]
names(SP500_liq) <- c("trade_volume")
## dealing with NA values
# check how many NA there are in the file
colSums(is.na(SP500_liq))
## basic statistic of the bank prices
hist(SP500_liq)
summary(SP500_liq)
dev.off()
chart_Series(SP500_liq$trade_volume)
# aggregating the data to the quarterly frequency
SP500_liq_q <- to.quarterly(SP500_liq)
SP500_liq_q <- SP500_liq_q[,4]
names(SP500_liq_q) <- c("trade_volume")
str(SP500_banks_q)
# taking 1st difference to deal with unit root
SP500_liq_diff <- diff.xts(SP500_banks_q)
SP500_liq_diff <- SP500_liq_diff[-1,]
names(SP500_liq_diff) <- c("trade_volume")
hist(SP500_liq_diff)
chart_Series(SP500_liq_diff$trade_volume)
## ADF test
# H_0: time series has unit root -> non-stationary
# original data -> data is non-stationary
adf_test_liq <- ur.df(SP500_liq_q$trade_volume,
type = c("drift"),
lags = 1)
summary(adf_test_liq)
# after the 1st difference -> data is stationary
adf_test_liq_diff <- ur.df(SP500_liq_diff$trade_volume,
type = c("drift"),
lags = 1)
summary(adf_test_liq_diff)
# merging the data
data_set <- merge(SP500_banks_diff, CPI_q_diff, SP500_liq_diff, CCI_q_diff,
EPU_q_diff)
View(data_set)
data_set <- data_set[(-1:-2),]
data_set <- data_set[-120,]
# removing NA
tail(data_set)
# removing NA
head(data_set)
# names for columns
names(data_set) <- c( "prices","CPI", "trade_volume", "CCI", "EPU")
# removing adf tests
rm(adf_test_banks_diff,adf_test_CPI, adf_test_CPI_diff, adf_test_liq,
adf_test_liq_diff,adf_test_banks,adf_test_CCI, adf_test_CCI_diff,EPU,
EPU_q, EPU_q_diff, EPU_xts)
# removing adf tests
rm(adf_test_banks_diff,adf_test_CPI, adf_test_CPI_diff, adf_test_liq,
adf_test_liq_diff,adf_test_banks,adf_test_CCI, adf_test_CCI_diff,EPU,
EPU_q, EPU_q_diff, EPU_xts)
adf_test <- list(adf_bank_0 = adf_test_banks,
adf_bank_1 = adf_test_banks_diff,
adf_CPI_0 = adf_test_CPI,
adf_CPI_1 = adf_test_CPI_diff,
adf_liq_0 = adf_test_liq,
adf_liq_1 = adf_test_liq_diff,
adf_CCI_0 = adf_test_CCI,
adf_CCI_1 = adf_test_CCI_diff)
rm(GSPC,CPI, CPI_q, CPI_q_diff, CPI_xts, SP500_banks, `SP500-40`, SP500_banks_diff,
SP500_banks_q, SP500_liq, SP500_liq_diff, SP500_liq_q, CCI, CCI_q, CCI_q_diff,
CCI_xts)
# Pearson correlation
pearson_corr <-cor(data_set)
pearson_corr
# selecting number of lags in the model
VARselect(data_set, # input data
lag.max = 12)   # maximum lag
# selecting number of lags in the model
VARselect(data_set, # input data
lag.max = 6)  # maximum lag
# running the VAR model
data_set_var2 <- VAR(data_set,
p = 2)
summary(data_set_var2)
# libraries
library(readxl)
library(dplyr)
library(xts)
# suppressing the warnings
options(warn = -1)
# setting the working directory
setwd("/Users/yuriisemenov/Documents/GitHub/RRcourse2023_project/Data")
# creating a vector with the names of the row files
data <- c("CPI", "DSPI", "HPI", "IR", "POP", "UNRATE", "CONF_2", "GDP")
# loading the data
for (i in data) {
filename <- paste0(i, ".xls")
file_path <- paste0("/Users/yuriisemenov/Documents/GitHub/RRcourse2023_project/Data/Raw_data/",
filename)
assign(i, read_excel(file_path))
}
file_path
# checking the structure of the data
data_2 <- list(CPI = CPI, DSPI = DSPI, HPI = HPI, IR = IR, POP = POP,
UNRATE = UNRATE, CONF_2 = CONF_2)
for (i in names(data_2)) {
x = str(data_2[[i]])
print(paste(i, as.character(x)))
}
## standardization of the data
# list to loop through
data_3 <- list(CPI = CPI, DSPI = DSPI, HPI = HPI, IR = IR, POP = POP,
UNRATE = UNRATE)
# steps for the loops:
# convert to data frame
# standardize the column names as "Date" + "name of the variable"
# convert to the xts object
# aggregate to quarter basis
for (name in names(data_3)) {
df <- as.data.frame(data_3[[name]])
names(df) <- c("Date", name)
df_xts <- xts(df[,2], order.by=df$Date)
df_q <- to.quarterly(df_xts)[, 4]
assign(paste0(name, "_q"), df_q)
}
# as CONF and GDP are downloaded with the quarterly frequency -> do adjustment
# outside the the main loop
# confidence
CONF_2 <- CONF_2[2]
CONF_2 <- data.frame(CONF_2)
# producing a vector that has all numeric values for CONF_2
CONF_2 <- as.numeric(unlist(CONF_2))
# GDP
GDP <- GDP[2]
GDP <- data.frame(GDP)
# producing a vector that has all numeric values for GDP_2
GDP <- as.numeric(unlist(GDP))
# creating the final dataset with all variables
dataset <- merge(CPI_q, DSPI_q, GDP, HPI_q, IR_q, POP_q, UNRATE_q, CONF_2)
names(dataset) <- c("CPI", "DSPI", "GDP", "HPI", "IR", "POP", "UNRATE", "CONF")
# removing unnecessary objects
rm(CPI, DSPI, GDP, HPI, IR, POP, UNRATE,CONF_2, data_2, data_3, df, df_q, df_xts, CPI_q,
DSPI_q, HPI_q, IR_q, POP_q, UNRATE_q, data, file_path, file_name,
i, name,x, filename)
# saving the preparing data file as an R object
save(dataset, file = "dataset")
print("dataset is created. You can proceed with the analysis")
str(data_set)
str(dataset)
